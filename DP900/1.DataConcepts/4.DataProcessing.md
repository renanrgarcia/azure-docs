# Data Processing
## Transactional Processing System
- Transactional processing involves the management of data transactions in a database.
- A transaction is a sequence of operations performed as a single logical unit of work.
- Key properties of transactional processing are defined by the ACID principles:
  - `Atomicity`: Ensures that all operations within a transaction are completed successfully or none at all.
  - `Consistency`: Guarantees that a transaction brings the database from one valid state to another.
  - `Isolation`: Ensures that concurrent transactions do not interfere with each other.
  - `Durability`: Guarantees that once a transaction is committed, its changes are permanent, even in the event of a system failure.

### Online Transaction Processing (OLTP)
- OLTP are the systems that manage real-time transaction data.
- They support a large number of short online transactions, such as order entry and financial transactions. 
- Key characteristics of OLTP systems include:
  - High concurrency: Many users can access the system simultaneously.
  - Low latency: Quick response times are essential for user satisfaction.
  - Data integrity: Ensuring accurate and consistent data is critical.

## Analytical Processing
- Analytical processing uses read-only (or read-mostly) systems that store vast volumes of historical data or business metrics.
- Architecture:
  - Operational data is extracted, transformed, and loaded (ETL) into a data lake for analysis.
  - Data is loaded into a schema of tables - typically in a Spark-based data lakehouse with tabular abstractions over files in the data lake, or a data warehouse with a fully relational SQL engine.
  - Data in the data warehouse may be aggregated and loaded into an online analytical processing (OLAP) model, or cube. Aggregated numeric values (measures) from fact tables are calculated for intersections of dimensions from dimension tables. For example, sales revenue might be totaled by date, customer, and product.
  - The data in the data lake, data warehouse, and analytical model can be queried to produce reports, visualizations, and dashboards.

### ETL (Extract, Transform, Load)
- ETL is a data integration process that involves:
  - `Extracting` data from various source systems.
  - `Transforming` the data into a suitable format for analysis.
  - `Loading` the transformed data into a target system, such as a data warehouse or data lake.

### ELT (Extract, Load, Transform)
- ELT is a variation of the ETL process that involves:
  - `Extracting` data from various source systems.
  - `Loading` the raw data into a target system, such as a data lake.
  - `Transforming` the data within the target system to prepare it for analysis.

### Data Lakes
- Data lakes are centralized repositories that allow you to store all your structured and unstructured data at any scale.

### Data Warehouse
- A data warehouse is store data in a relational schema that is optimized for read operations â€“ primarily queries to support reporting and data visualization.
- `Data Lakehouses` are a new architectural paradigm that combines the best features of data lakes and data warehouses.
  - They provide the flexibility and scalability of data lakes while offering the performance and management features of data warehouses.

## OLAP (Online Analytical Processing)
- OLAP systems are designed to handle complex queries and provide multidimensional analysis of business data.
- Key features of OLAP include:
  - **Multidimensional data model**: Data is organized into cubes, allowing for fast retrieval and analysis across multiple dimensions (e.g., time, geography, product).
  - **Aggregations**: OLAP systems pre-compute and store aggregated data, improving query performance.
  - **Drill-down and roll-up**: Users can explore data at different levels of granularity, from high-level summaries to detailed transactions.